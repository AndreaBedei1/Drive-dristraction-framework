\documentclass[a4paper,11pt]{article}
\usepackage[margin=2.2cm]{geometry}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{hyperref}

\title{Simple Explanation of Fitness-to-Drive}
\author{Easy-to-read version}
\date{\today}

\begin{document}
\maketitle

\section{Super simple idea}
Imagine a digital co-driver.
This co-driver watches what happens while you drive and tries to answer one question:

\begin{center}
\textbf{``In the next second, how likely is a driving error?''}
\end{center}

The file \texttt{ftd\_predictor.py} builds this co-driver.

\section{What Fitness-to-Drive means}
\textbf{Fitness-to-Drive} means: \textit{how ready you are to drive safely right now}.

In the code:
\begin{center}
\texttt{fitness\_to\_drive = 1 - error\_probability}
\end{center}

Example:
\begin{itemize}
    \item if \texttt{error\_probability = 0.20}, then \texttt{fitness\_to\_drive = 0.80}.
    \item if \texttt{error\_probability = 0.70}, then \texttt{fitness\_to\_drive = 0.30}.
\end{itemize}

So:
\begin{itemize}
    \item low error probability = safer driving condition;
    \item high error probability = riskier driving condition.
\end{itemize}

\section{What data it uses}
The program reads 4 CSV files:
\begin{enumerate}
    \item when distractions happened;
    \item when errors happened in distraction runs;
    \item errors in normal driving (baseline);
    \item how many baseline driving seconds we have.
\end{enumerate}

In simple words: it looks at \textbf{when you are distracted}, \textbf{when you make errors}, and \textbf{what happens when there are no special distractions}.

\section{The 9 model features (simple explanation)}
The model uses 9 clues (features). Each clue is a number that helps estimate risk.

\begin{enumerate}
    \item \texttt{distraction\_active}\\
    Says if a distraction is active now: 1 = yes, 0 = no.\\
    Computed from \texttt{timestamp\_start} and \texttt{timestamp\_end} in \texttt{Dataset Distractions\_distraction.csv}.

    \item \texttt{time\_since\_last\_dist}\\
    Says how many seconds passed since the last distraction ended.\\
    Also computed from distraction window timestamps.

    \item \texttt{model\_prob}\\
    Confidence from the arousal model.\\
    For error samples it comes from \texttt{model\_prob} in \texttt{Dataset Errors\_distraction.csv}.\\
    For distraction negatives it comes from \texttt{model\_prob\_start} or \texttt{model\_prob\_end}.

    \item \texttt{model\_pred\_enc}\\
    Arousal label transformed into a number (encoding).\\
    Built from \texttt{model\_pred}, \texttt{model\_pred\_start}, \texttt{model\_pred\_end}.

    \item \texttt{emotion\_prob}\\
    Confidence of the emotion classifier.\\
    Built from \texttt{emotion\_prob} (errors) or \texttt{emotion\_prob\_start/end} (distractions).

    \item \texttt{emotion\_label\_enc}\\
    Emotion label transformed into a number (encoding).\\
    Built from \texttt{emotion\_label}, \texttt{emotion\_label\_start}, \texttt{emotion\_label\_end}.

    \item \texttt{baseline\_error\_rate}\\
    The driver's normal risk when driving without special distractions.\\
    Computed from:
    \begin{itemize}
        \item \texttt{Dataset Errors\_baseline.csv} (how many baseline errors);
        \item \texttt{Dataset Driving Time\_baseline.csv} (how many baseline driving seconds).
    \end{itemize}

    \item \texttt{speed\_kmh}\\
    Car speed at that moment.\\
    From \texttt{speed\_kmh} (error rows) or \texttt{speed\_kmh\_start/end} (distraction rows).

    \item \texttt{steer\_angle\_deg}\\
    Steering wheel angle in degrees.\\
    From \texttt{steer\_angle\_deg} (error rows) or \texttt{steer\_angle\_deg\_start/end} (distraction rows).
\end{enumerate}

In short: the model uses \textbf{distraction status}, \textbf{recovery time}, \textbf{emotion/arousal state}, \textbf{personal baseline risk}, and \textbf{vehicle motion} (speed and steering).

\section{How it works, step by step}
\begin{enumerate}
    \item \textbf{Read data.}
    \item \textbf{Check data quality.} If data has serious issues, stop.
    \item \textbf{Estimate normal risk.} Each driver may have a different baseline risk.
    \item \textbf{Build training examples.}
    \begin{itemize}
        \item positive examples = seconds where an error happened;
        \item negative examples = seconds where no error happened.
    \end{itemize}
    \item \textbf{Train the model} (XGBoost).
    \item \textbf{Test on unseen drivers} to check real generalization.
    \item \textbf{Calibrate probabilities} to make numbers more reliable.
    \item \textbf{Save the model} in a \texttt{.pkl} file.
\end{enumerate}

\section{How error probability is computed}
For a new driving instant, the model looks at:
\begin{itemize}
    \item is distraction active or not;
    \item how long since the last distraction ended;
    \item arousal/emotion signals;
    \item speed and steering angle;
    \item the driver's baseline risk.
\end{itemize}

Then it does two steps:
\begin{enumerate}
    \item compute a raw probability (\texttt{raw\_prob});
    \item adjust it with calibration (\texttt{calibrated\_prob}).
\end{enumerate}

The final error probability is the calibrated one.

\section{Why calibration is used}
A model can output ``0.70'', but that number may not be perfectly honest.

Calibration makes probabilities more trustworthy:
\begin{itemize}
    \item if it says 0.30, it should behave close to 30\% risk;
    \item if it says 0.80, it should really mean very high risk.
\end{itemize}

\section{Metrics explained for a child}
Metrics are model report-card grades.

\begin{itemize}
    \item \textbf{Precision}: when the model says ``warning!'', how often it is correct.
    \item \textbf{Recall}: how many real errors it catches.
    \item \textbf{F1}: one score that combines precision and recall.
    \item \textbf{AUC-PR}: how good it is at finding rare errors.
    \item \textbf{AUC-ROC}: how well it separates safe vs risky moments.
    \item \textbf{Brier}: how good probability numbers are (lower is better).
    \item \textbf{Log-Loss}: strong penalty for very wrong confident probabilities.
    \item \textbf{MCC/Kappa}: robust scores when classes are imbalanced.
    \item \textbf{Specificity}: how well it recognizes safe seconds.
    \item \textbf{NPV}: when it says ``safe'', how often it is truly safe.
\end{itemize}

The code also uses \textbf{bootstrap}: it repeats metric computation many times to check stability.

\section{What the final function returns}
The function \texttt{predict\_fitness(...)} returns:
\begin{itemize}
    \item \texttt{error\_probability}: risk of error in the next second;
    \item \texttt{fitness\_to\_drive}: how fit/safe the driver is right now;
    \item \texttt{alert}: true/false if risk is above threshold;
    \item \texttt{input\_warnings}: warnings for invalid or unusual inputs.
\end{itemize}

\section{Final sentence}
This system does not only say ``error / no error''.
It also estimates \textbf{how much risk there is}, and tries to make that number reliable using data checks, validation, calibration, and multiple metrics.

\end{document}
